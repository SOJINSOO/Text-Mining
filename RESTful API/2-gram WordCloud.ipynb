{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 바이그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패키지 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import PorterStemmer  # 어근추출\n",
    "from nltk.tokenize import RegexpTokenizer  # 정규표현식을 사용하여 단어 토큰화를 제공\n",
    "from nltk.corpus import stopwords  # 불용어 정의\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 완료된 Pickle 파일 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 전처리가 완료된 pickle파일을 불러온다\n",
    "with open(r\"C:\\User\\SOJINSOO\\여행고수\\df_food_review.txt\",\"rb\") as f:\n",
    "    df_food_review = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 단어 묶음으로 변환\n",
    "def english_tokenizer(lines):\n",
    "    tokenizer = RegexpTokenizer('[\\w]+')  # \\w : 단어 영문자+숫자+_(밑줄) [0-9a-zA-Z_]\n",
    "\n",
    "    stop_words = stopwords.words('english')  # 불용어 정의\n",
    "\n",
    "    ### 영단어 전처리 과정\n",
    "    words =lines.lower()  # 모든 단어를 소문자로 변환\n",
    "    tokens = tokenizer.tokenize(words)  # 단어 단위로 토큰화\n",
    "    stopped_tokens = [i for i in list((tokens)) if not i in stop_words]  # 불용어 제거\n",
    "    words = [i for i in stopped_tokens if len(i)>1]  # 한 글자 제거\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud 객체 생성 및 레이아웃 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud 객체 생성 및 레이아웃 설정\n",
    "wordcloud = WordCloud(\n",
    "#font_path = font_path,\n",
    "width = 800,\n",
    "height = 800,\n",
    "background_color = \"white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy array로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy 배열로 변환\n",
    "def to_array(self):\n",
    "    \"\"\"Convert to numpy array.\n",
    "    Returns\n",
    "    -------\n",
    "    image : nd-array size (width, height, 3)\"\n",
    "        Word cloud image as numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.array(self.to_image())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram 빈도 분석 및 WordCloud Image 생성, 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#음식점 한곳씩 반복문을 돌린다.\n",
    "for food in df_food_review:\n",
    "    try:\n",
    "        line = []\n",
    "        bi_gram = []\n",
    "        words = english_tokenizer(food['content'])#음식점의 리뷰 문장을 단어로 변환한다.\n",
    "        bi_tokens = ngrams(words, 2)#변환한 단어를 ngrams함수를 이용해서 2-gram으로 변환한다.\n",
    "        for i in bi_tokens: #변환된 bi_gram을 한 리스트에 집어넣는다\n",
    "            bi_gram.append(i)\n",
    "        sort = sorted(bi_gram, key = lambda x : x[0]) #정렬\n",
    "        count = Counter(sort) #카운팅한다. (빈도수 freq를 위함)\n",
    "        dict = { #카운팅한 bi_gram단어와 빈도수가 들어갈 dict 선언\n",
    "            'term1' : [],\n",
    "            'term2' : [],\n",
    "            'freq' : []\n",
    "        }\n",
    "\n",
    "        for i,j in count.items(): #bi_gram단어와 빈도수를 입력한다.\n",
    "            dict['term1'].append(list(i)[0])\n",
    "            dict['term2'].append(list(i)[1])\n",
    "            dict['freq'].append(j)\n",
    "            \n",
    "        bi_gram = pd.DataFrame(dict) #입력한 사전을 DataFrame으로 변환\n",
    "        bi_gram.sort_values('freq',ascending=False,inplace=True) #정렬\n",
    "        bi_gram['term'] = bi_gram['term1'] + ' ' + bi_gram['term2'] #두 단어를 하나의 단어로 합친다.\n",
    "        \n",
    "        bi_gram = bi_gram[['term','freq']] #필요한 컬럼만 남긴다\n",
    "        sample = bi_gram.set_index('term').to_dict() #다시 딕셔너리로 변환\n",
    "        wordcloud = wordcloud.generate_from_frequencies(sample['freq'])#wordcloud로 변환한다.\n",
    "        array = wordcloud.to_array()#배열로 변환\n",
    "\n",
    "        fig = plt.figure(figsize=(10,10))#figure 객체 생성 및 사이즈 설정\n",
    "        plt.imshow(array, interpolation=\"bilinear\")  # 보간법 = 쌍선형 보간법\n",
    "        fig.savefig(r'C:\\User\\SOJINSOO\\여행고수\\bi_gram\\save\\{}.png'.format(str(file_name))) # 이미지 파일을 생성한다.\n",
    "        plt.close(fig) # 생성후 다음 반복문을 위해, 메모리 낭비를 최소화해야하므로 close한다.\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
